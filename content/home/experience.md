+++
# Experience widget.
widget = "experience"  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 40  # Order that this section will appear.

title = "Experience"
#subtitle = "PREVIOUS ASSOCIATIONS THAT HELPED TO GATHER EXPERIENCE"

# Date format for experience
#   Refer to https://sourcethemes.com/academic/docs/customization/#date-format
date_format = "Jan 2006"

# Experiences.
#   Add/remove as many `[[experience]]` blocks below as you like.
#   Required fields are `title`, `company`, and `date_start`.
#   Leave `date_end` empty if it's your current employer.
#   Begin/end multi-line descriptions with 3 quotes `"""`.

[[experience]]
  title = "Senior Software Engineer - Niche"
  company = "Legato Health Technologies, Anthem Inc."
  company_url = ""
  location = "Bangalore, India"
  date_start = "2020-10-05"
  date_end = "2021-08-19"
  description = """ 
  Highlights :
  
* Transformed segmentation data from S3, Athena to the reporting layer in Snowflake.
* Built data pipelines in AWS leveraging services S3, RDS, Athena, Step functions, and EMR.
* Migrated data from the on-premise Hadoop cluster to AWS and Snowflake.

Technologies: 

AWS: RDS, S3, EMR, Glue, Spark, Scala, Hive, Hadoop, Snowflake, Git, Bitbucket, Maven
  """

[[experience]]
  title = "Software Engineer - Big Data"
  company = "Legato Health Technologies, Anthem Inc."
  company_url = ""
  location = "Bangalore, India"
  date_start = "2018-06-04"
  date_end = "2020-10-04"
  description = """
  Highlights :
  
* Engaged primarily in developing spark Scala code involving RDDâ€™s, dataframes, and SparkSQL.
* Developed shell scripts to process 1.5 TB CSV, Parquet data from inbound to the outbound layer for generating Tableau reports.
* Developed fully automated CI/CD pipelines using Bamboo to migrate Unix items and ETL jars into pre-prod and prod environments removing any manual effort.

Innovations & Enhancements:

* Automated validation reports post-migration bringing down costs by 90%.
* Improved runtime from 18 hours to 9.5 hours by refactoring Spark Scala ETL code.
* Refactored tables to use parquet formats, snappy compressions, and include partitions.

* Improved efficiency and turnaround time from 6 hours to 1.5 hours by automating data quality and validity checks between Hive and SQL server loads.

* Developed automation scripts for SIT in Spark Scala to assess quality, validity, counts of inbound data files & tables to remove manual effort and intervention.

Technologies: 

Spark, Scala, Hive, Impala, Hadoop, Unix, Shell scripting, Control M, Bamboo, Git, Bitbucket, Maven, Eclipse, Cloudera distribution
  """

[[experience]]
  title = "Software Engineer - Big Data & Hadoop Developer"
  company = "Middle East Management Consultancy and Marketing"
  company_url = ""
  location = "Muscat, Oman"
  date_start = "2016-06-01"
  date_end = "2018-05-31"
  description = """
Highlights :
* Shipped and delivered product end to end.
* Implemented SQOOP for massive dataset transfer between the Hadoop file system and RDBMS.
* Involved in the design and creation of partitioned table DDLs in Hive.
* Worked on performance tuning, analysis, and response time reduction techniques in SQL and Sqoop.
* Worked with different file formats such as CSV, Parquet, and snappy compressed files.
* Processed delimited data using Spark SQL to build pipelines from landing zone to outbound layer.
  """

[[experience]]
  title = "Practice School Student/ Researcher"
  company = "Manipal Institute of Technology"
  company_url = ""
  location = "Manipal, India"
  date_start = "2016-01-01"
  date_end = "2016-05-19"
  description = """Central Data Repository for MIT, Manipal: 
  
  Delivered a web application with its main objectives to serve as a means of data entry, to collect the required data, to analyze the given data, and finally to generate reports dynamically according to the custom report format requirements of the user. The data was loaded from the databases using Sqoop and analyzed using a Hadoop cluster. The reports are generated after querying using Hive and displayed in the web application."""
  
[[experience]]
  title = "Software Development Intern"
  company = "CGI Information Systems and Management Consultants Pvt. Ltd"
  company_url = ""
  location = "Manipal, India"
  date_start = "2015-05-01"
  date_end = "2015-07-30"
  description = """Project Management System: 
  
  Developed a web application that enabled the interaction between different users of different departments and their respective projects while accessing their functions on a large scale."""  
+++
